{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advise: the Python cells in this notebook are not (yet) prepared to be run, as are only included for exemplifying purposes. Last revision: 13th March."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "by Pau Fabregat\n",
    "\n",
    "### Table of contents\n",
    "1. [Prepare the model](#1)\n",
    "    1. [Models from torchvision](#11)\n",
    "    2. [Custom model](#12)\n",
    "2. [Obtaining the quantized checkpoint](#2)\n",
    "    1. [Post-Training Static](#21)\n",
    "    2. [Quantization-Aware Training (QAT) with PyTorch Lightning](#22)\n",
    "    3. [Quantization-Aware Training (QAT) with PyTorch](#23)\n",
    "3. [Loading the quantized checkpoint](#3)\n",
    "4. [Usual errors](#4)\n",
    "5. [Sources and useful links](#5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='1'/>\n",
    "\n",
    "## 1. Prepare the model\n",
    "Some changes have to be done in the model in order to quantize it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='11'/>\n",
    "\n",
    "### 1.1. Models from torchvision\n",
    "The torchvision.models.quantization contains prepared versions for quantization of some of the most-used models, such as ResNet, MobileNet, or ShuffleNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.quantization import shufflenetv2 as qshufflenet\n",
    "model = qshufflenet.shufflenet_v2_x1_0(pretrained=True, quantize=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train this model in an usual manner. If quantize=True, we are given the already int8 quantized model. We do not want this as we want to keep on training the model with our own data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='12'/>\n",
    "\n",
    "### 1.2. Custom model <a name=\"1.2\"></a>\n",
    "If the model we are using is not included in the torchvision.models.quantization, we need to perform some manual modifications. The following block has been extracted from the PyTorch documentation:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It is necessary to currently make some modifications to the model definition\n",
    "prior to Eager mode quantization. This is because currently quantization works on a module\n",
    "by module basis. Specifically, for all quantization techniques, the user needs to:  \n",
    "> 1. Convert any operations that require output requantization (and thus have additional parameters) from functionals to module form (for example, using ``torch.nn.ReLU`` instead of ``torch.nn.functional.relu``).  \n",
    "> 2. Specify which parts of the model need to be quantized either by assigning ``.qconfig`` attributes on submodules or by specifying ``qconfig_mapping``. For example, setting ``model.conv1.qconfig = None`` means that the ``model.conv`` layer will not be quantized, and setting ``model.linear1.qconfig = custom_qconfig`` means that the quantization settings for ``model.linear1`` will be using ``custom_qconfig`` instead of the global qconfig.    \n",
    ">\n",
    ">For static quantization techniques which quantize activations, the user needs to do the following in addition:  \n",
    "> 1. Specify where activations are quantized and de-quantized. This is done using `torch.ao.quantization.QuantStub` and `torch.ao.quantization.DeQuantStub` modules.  \n",
    "> 2. Use `torch.ao.nn.quantized.FloatFunctional` to wrap tensor operations that require special handling for quantization into modules. Examples are operations like ``add`` and ``cat`` which require special handling to determine output quantization parameters.  \n",
    "> 3. Fuse modules: combine operations/modules into a single module to obtain higher accuracy and performance. This is done using the `torch.ao.quantization.fuse_modules` API, which takes in lists of modules to be fused. We currently support the following fusions: [Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, in the multitask model, it was convenient not to quantize the last linear layer of the road classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.model.model.road_head[-1].linear.qconfig = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SiLU activation function used in YOLOv5 can't be quantized. Although it admits a quantized tensor (likely because dequantization is done internally), using dequant and quant before and after this function is faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.quant = torch.ao.quantization.QuantStub()\n",
    "self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "x = self.quant(nn.SiLU(self.dequant(x))) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three 'simple' operations of addition, multiplication, and concatenation have to be adapted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.ff = torch.ao.nn.quantized.FloatFunctional()\n",
    "\n",
    "# Adapt addition\n",
    "# Old:\n",
    "x + y\n",
    "# New:\n",
    "self.ff.add(x, y)\n",
    "\n",
    "# Adapt multiplication\n",
    "# Old:\n",
    "x * y\n",
    "# New:\n",
    "self.ff.mul(x, y)\n",
    "\n",
    "# Adapt concatenation\n",
    "# Old:\n",
    "torch.cat(x, y)\n",
    "# New:\n",
    "self.ff.cat(x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, the Bottleneck module in the YOLOv5 has to be converted from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    # Standard bottleneck\n",
    "    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):\n",
    "        super().__init__()\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, 1, 1)\n",
    "        self.cv2 = Conv(c_, c2, 3, 1, g=g)\n",
    "        self.add = shortcut and c1 == c2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    # Standard bottleneck\n",
    "    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):\n",
    "        super().__init__()\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, 1, 1)\n",
    "        self.cv2 = Conv(c_, c2, 3, 1, g=g)\n",
    "        self.add = shortcut and c1 == c2\n",
    "        self.ff = torch.ao.nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff.add(x, self.cv2(self.cv1(x))) if self.add else self.cv2(self.cv1(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='2'/>\n",
    "\n",
    "## 2. Obtaining the quantized checkpoint <a name=\"2\"></a>\n",
    "PyTorch allows three types of quantization:\n",
    "1. Post-training dynamic quantization (PTQ-dynamic). This method does not allow the quantization of convolutional layers, so it is not suitable for the project.\n",
    "2. Post-training static quantization (PTQ-static).\n",
    "3. Quantization-aware training (QAT)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='21'/>\n",
    "\n",
    "### 2.1. Post-Training Static <a name=\"2.1\"></a>\n",
    "One of its key parts is the calibration. The general workflow is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.qconfig = torch.ao.quantization.get_default_qconfig(\"fbgemm\")\n",
    "model = torch.ao.quantization.prepare(model)\n",
    "calibrate(model_fp32_prepared, dataloader)\n",
    "model = torch.ao.quantization.convert(model)\n",
    "torch.save(model.state_dict(), \"quantized_checkpoint.ckpt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading a quantized model, we will use a quite similar procedure. In the previous block, we have to define the calibrate function (using ~100 samples is said to be enough), and we could fuse some layers (for instance, convolutions and batchnorms):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the non-quantized checkpoint\n",
    "# Use CPU, not GPU\n",
    "ckpt = torch.load(\"checkpoint.ckpt\")\n",
    "model.load_state_dict(ckpt, strict=False)\n",
    "\n",
    "# Set model to eval and define the qconfig\n",
    "model.eval()\n",
    "model.qconfig = torch.ao.quantization.get_default_qconfig(\"fbgemm\")\n",
    "\n",
    "# Fuse layers (optional)\n",
    "for m in model.modules():\n",
    "    if type(m) == Conv:\n",
    "        torch.ao.quantization.fuse_modules(m, ['conv', 'bn'], inplace=True)\n",
    "\n",
    "# Prepare for static quantization\n",
    "model = torch.ao.quantization.prepare(model)\n",
    "\n",
    "# Calibrate the model. Use min 100 samples.\n",
    "def calibrate(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image, target in dataloader:\n",
    "            model(image)\n",
    "print(\"Calibrating model... please wait...\")\n",
    "calibrate(model_fp32_prepared, dataloader)  # Define a dataloader\n",
    "\n",
    "# Quantize and save the model\n",
    "model = torch.ao.quantization.convert(model)\n",
    "torch.save(model.state_dict(), \"quantized_checkpoint.ckpt\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='22'/>\n",
    "\n",
    "### 2.2. Quantization-aware Training (QAT) with PyTorch Lightning <a name=\"2.2\"></a>\n",
    "The PyTorch Lightning (PL) library has its custom QAT callback. If we pass this callback to the PL trainer, the QAT is done automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import QuantizationAwareTraining\n",
    "callbacks = {}\n",
    "\n",
    "# We define the PL QAT callback\n",
    "qat_callback = QuantizationAwareTraining(qconfig='fbgemm', observer_type='average')\n",
    "callbacks[\"qat_callback\"] = qat_callback\n",
    "\n",
    "# Optionally add more callbacks (LearningRateMonitor, ModelCheckpoint, EarlyStopping, ...)\n",
    "\n",
    "# We pass this callbacks dict to the trainer\n",
    "# https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html\n",
    "trainer = pl.Trainer.from_argparse_args(callbacks=list(my_callbacks.values()))\n",
    "trainer.fit(module, datamodule=data_module)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='23'/>\n",
    "\n",
    "### 2.3. Quantization-aware Training (QAT) with PyTorch <a name=\"2.3\"></a>\n",
    "TO DO..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='3'/>\n",
    "\n",
    "## 3. Loading the quantized checkpoint <a name=\"3\"></a>\n",
    "Once the quantized checkpoint has been saved, loading the model is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model for quantization\n",
    "model.eval()\n",
    "model.qconfig = torch.ao.quantization.get_default_qconfig(\"fbgemm\")\n",
    "model = torch.ao.quantization.prepare(model)\n",
    "model = torch.ao.quantization.convert(model)\n",
    "\n",
    "# Load the quantized checkpoint\n",
    "# Use CPU, not GPU\n",
    "ckpt = torch.load(\"quantized_checkpoint.ckpt\")\n",
    "model.load_state_dict(ckpt, strict=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='4'/>\n",
    "\n",
    "## 4. Usual errors <a name=\"4\"></a>\n",
    "Passing a non-quantized Tensor into a quantized kernel:\n",
    "\n",
    "RuntimeError: Could not run 'quantized::some_operator' with arguments from the 'CPU' backend...\n",
    "\n",
    "Solution: we have to quantize the tensor before using the operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.quant = torch.quantization.QuantStub()\n",
    "x = self.quant(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing a quantized Tensor into a non-quantized kernel:\n",
    "\n",
    "RuntimeError: Could not run 'aten::thnn_conv2d_forward' with arguments from the 'QuantizedCPU' backend.\n",
    "\n",
    "Solution: we have to dequantize the tensor before using the operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.dequant = torch.quantization.DeQuantStub()\n",
    "x = self.dequant(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='5'/>\n",
    "\n",
    "## 5. Sources and useful links <a name=\"5\"></a>\n",
    "https://pytorch.org/blog/introduction-to-quantization-on-pytorch/  \n",
    "https://pytorch.org/docs/stable/quantization.html  \n",
    "https://pytorch.org/tutorials/recipes/quantization.html  \n",
    "https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html  \n",
    "https://pytorch-lightning.readthedocs.io/en/stable/advanced/pruning_quantization.html  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
